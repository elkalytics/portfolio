#' Anomaly Detection using Time Series Analysis
#'
#' This function demonstrates how to detect anomalies in a time series dataset using time series analysis.
#'
#' @author [Your Name Here]
#' @param time_series_data a numeric vector representing the time series data
#' @return a plot of the original time series data with anomalies highlighted
#' @export
#' @examples
#' set.seed(123)
#' time_series_data <- ts(rnorm(100), start=c(2010,1), frequency=12)
#' anomaly_detection(time_series_data)
anomaly_detection <- function(time_series_data) {
# Load the necessary packages
library(forecast)
library(ggplot2)
# Step 1: Load and preprocess the data
# Remove missing values
time_series_data <- time_series_data[complete.cases(time_series_data), ]
# Step 2: Decomposition
# Decompose the time series data
decomposed_data <- stl(time_series_data, s.window="periodic")
# Step 3: Identify outliers
# Plot a scatter plot of the residual component
ggplot(data.frame(time = 1:length(decomposed_data$time.series[, "remainder"]),
value = decomposed_data$time.series[, "remainder"]),
aes(x = time, y = value)) +
geom_point() +
xlab("Time") + ylab("Residuals") +
ggtitle("Scatter Plot of Residuals")
# Step 4: Statistical tests
# Use the Z-score test to detect anomalies
z_score <- abs(scale(decomposed_data$time.series[, "remainder"]))
anomalies <- which(z_score > 3)
# Step 5: Time series models
# Fit an ARIMA model to the data
model <- auto.arima(time_series_data)
# Step 6: Visualization
# Plot the original time series data with anomalies highlighted
ggplot(data.frame(time = 1:length(time_series_data), value = time_series_data),
aes(x = time, y = value)) +
geom_line() +
geom_point(data = data.frame(time = anomalies, value = time_series_data[anomalies]),
color = "red") +
ggtitle("Anomalies in Time Series Data")
}
# Generate an example time series data set
set.seed(123)
time_series_data <- ts(rnorm(100), start=c(2010,1), frequency=12)
# Add an anomaly at time point 30
time_series_data[30] <- time_series_data[30] + 5
#' Load the required packages
#'
#' @import randomForest
#' @import caret
#' @import ggplot2
#'
#' @export
library(randomForest)
library(caret)
library(ggplot2)
#' Generate a fake dataset with 3 classes
#'
#' @param n Number of samples
#' @return A data frame with n rows and 4 columns, including x1, x2, x3, and y
#'
#' @export
set.seed(123)
n <- 1000
x1 <- rnorm(n, 0, 1)
x2 <- rnorm(n, 0, 1)
x3 <- rnorm(n, 0, 1)
y <- factor(sample(c("A", "B", "C"), n, replace = TRUE))
data <- data.frame(x1, x2, x3, y)
#' Check for missing values
#'
#' @param data The data frame to check for missing values
#' @return The number of missing values
#'
#' @export
sum(is.na(data))
#' Split the data into training and testing sets
#'
#' @param data The data frame to split
#' @param p The proportion of the data to use for training
#' @return A list of two data frames: the training set and the testing set
#'
#' @export
set.seed(123)
trainIndex <- createDataPartition(data$y, p = 0.8, list = FALSE)
train <- data[trainIndex, ]
test <- data[-trainIndex, ]
#' Make sure factor levels match between train and test
#'
#' @param train The training set
#' @param test The testing set
#'
#' @export
levels(test$y) <- levels(train$y)
#' Build the random forest model
#'
#' @param formula A formula specifying the model
#' @param data The data frame containing the data to fit the model
#' @param ntree The number of trees to grow
#' @param mtry The number of variables to sample as candidates at each split
#' @return A random forest model object
#'
#' @export
set.seed(123)
rf_model <- randomForest(y ~ ., data = train, ntree = 500, mtry = sqrt(ncol(train)-1))
#' Check the variable importance
#'
#' @param model The random forest model
#'
#' @export
varImpPlot(rf_model)
#' Make predictions on the test set
#'
#' @param model The random forest model
#' @param newdata The data frame containing the new data to predict on
#' @return A vector of predicted classes
#'
#' @export
rf_pred <- predict(rf_model, newdata = test)
#' Evaluate the model performance
#'
#' @param actual The actual classes
#' @param predicted The predicted classes
#' @return A confusion matrix object
#'
#' @export
confusionMatrix(rf_pred, test$y)
#' Load required packages
#'
#' This function loads the required packages for the analysis.
#'
#' @importFrom randomForest randomForest
#' @importFrom caret createDataPartition
#' @importFrom tidyverse ggplot
#' @importFrom car vif
#'
#' @examples
#' library(randomForest)
#' library(caret)
#' library(tidyverse)
#' library(car)
library(randomForest)
library(caret)
library(tidyverse)
library(car)
#' Generate fake data
#'
#' This function generates fake data for the analysis.
#'
#' @param n The number of observations to generate.
#'
#' @return A data frame containing the generated data.
#'
#' @examples
#' set.seed(123)
#' mydata <- generate_fake_data(n = 1000)
#'
#' trainIndex <- createDataPartition(mydata$y, p = .8, list = FALSE)
#' train <- mydata[trainIndex, ]
#' test <- mydata[-trainIndex, ]
generate_fake_data <- function(n) {
set.seed(123)
x1 <- rnorm(n, mean = 0, sd = 1)
x2 <- rnorm(n, mean = 0, sd = 1)
x3 <- rnorm(n, mean = 0, sd = 1)
y <- 2 * x1 + 3 * x2 - 5 * x3 + rnorm(n, mean = 0, sd = 1)
data.frame(x1, x2, x3, y)
}
mydata <- generate_fake_data(n = 1000)
trainIndex <- createDataPartition(mydata$y, p = .8, list = FALSE)
train <- mydata[trainIndex, ]
test <- mydata[-trainIndex, ]
#' Use random forest to perform feature selection
#'
#' This function uses random forest to perform feature selection on the given dataset.
#'
#' @param data A data frame containing the input variables and response variable.
#' @param response The name of the response variable.
#' @param importance A logical value indicating whether or not to compute variable importance measures.
#'
#' @return A randomForest object containing the fitted model.
#'
#' @examples
#' set.seed(123)
#' n <- 1000
#' mydata <- generate_fake_data(n)
#'
#' trainIndex <- createDataPartition(mydata$y, p = .8, list = FALSE)
#' train <- mydata[trainIndex, ]
#' test <- mydata[-trainIndex, ]
#'
#' rf_model <- perform_feature_selection(data = train, response = "y", importance = TRUE)
#' varImpPlot(rf_model, sort = TRUE, main = "Random Forest Feature Importance")
perform_feature_selection <- function(data, response, importance) {
randomForest(as.formula(paste(response, "~ .")), data = data, importance = importance)
}
rf_model <- perform_feature_selection(data = train, response = "y", importance = TRUE)
varImpPlot(rf_model, sort = TRUE, main = "Random Forest Feature Importance")
#' Check linearity using a scatterplot
#'
#' This function checks the linearity assumption using a scatterplot.
#'
#' @param data A data frame containing the input variables and response variable.
#' @param x The name of the predictor variable to plot on the x-axis.
#' @param y The name of the response variable to plot on the y-axis.
#'
#' @examples
#' set.seed(123)
#' n <- 1000
#' mydata <- generate_fake_data(n)
#'
#' trainIndex <- createDataPartition(mydata$y, p = .8, list = FALSE)
#' train <- mydata[trainIndex, ]
#' test <- mydata[-trainIndex, ]
#'
#' check_linearity(train, "x1", "y")
check_linearity <- function(data, x, y) {
ggplot(data, aes_string(x = x, y = y)) +
geom_point() +
ggtitle(paste0("Scatterplot of ", y, " vs. ", x)) +
xlab(x) +
ylab(y)
}
check_linearity(train, "x1", "y")
#' Check normality using a histogram
#'
#' This function checks the normality assumption using a histogram.
#'
#' @param data A data frame containing the response variable.
#' @param y The name of the response variable.
#'
#' @examples
#' set.seed(123)
#' n <- 1000
#' mydata <- generate_fake_data(n)
#'
#' trainIndex <- createDataPartition(mydata$y, p = .8, list = FALSE)
#' train <- mydata[trainIndex, ]
#' test <- mydata[-trainIndex, ]
#'
#' check_normality(train, "y")
check_normality <- function(data, y) {
ggplot(data, aes_string(x = y)) +
geom_histogram() +
ggtitle(paste0("Histogram of ", y)) +
xlab(y) +
ylab("Count")
}
check_normality(train, "y")
#' Check normality with Q-Q plot
#'
#' This function checks the normality assumption using a Q-Q plot.
#'
#' @param residuals The residuals from the fitted model.
#'
#' @examples
#' set.seed(123)
#' n <- 1000
#' mydata <- generate_fake_data(n)
#'
#' trainIndex <- createDataPartition(mydata$y, p = .8, list = FALSE)
#' train <- mydata[trainIndex, ]
#' test <- mydata[-trainIndex, ]
#'
#' rf_model <- perform_feature_selection(data = train, response = "y", importance = TRUE)
#' residuals <- train$y - rf_model$predicted
#'
#' check_normality_qq(residuals)
check_normality_qq <- function(residuals) {
qqPlot(residuals, main = "Q-Q plot of residuals")
}
rf_model <- perform_feature_selection(data = train, response = "y", importance = TRUE)
residuals <- train$y - rf_model$predicted
check_normality_qq(residuals)
#' Check homoscedasticity using a residuals vs. fitted plot
#'
#' This function checks the homoscedasticity assumption using a residuals vs. fitted plot.
#'
#' @param data A data frame containing the response variable and predicted values.
#' @param predicted The name of the predicted variable.
#' @param residuals The name of the residuals variable.
#'
#' @examples
#' set.seed(123)
#' n <- 1000
#' mydata <- generate_fake_data(n)
#'
#' trainIndex <- createDataPartition(mydata$y, p = .8, list = FALSE)
#' train <- mydata[trainIndex, ]
#' test <- mydata[-trainIndex, ]
#'
#' rf_model <- perform_feature_selection(data = train, response = "y", importance = TRUE)
#' residuals <- train$y - rf_model$predicted
#'
#' check_homoscedasticity(train, "y", "residuals")
check_homoscedasticity <- function(data, predicted, residuals) {
ggplot(data, aes_string(x = predicted, y = residuals)) +
geom_point() +
ggtitle("Residuals vs. Fitted") +
xlab("Fitted values") +
ylab("Residuals")
}
check_homoscedasticity(train, "rf_model$predicted", "residuals")
#' Check multicollinearity using VIF
#'
#' This function checks the multicollinearity assumption using VIF.
#'
#' @param data A data frame containing the input variables and response variable.
#' @param formula A formula specifying the linear regression model to fit.
#'
#' @examples
#' set.seed(123)
#' n <- 1000
#' mydata <- generate_fake_data(n)
#'
#' trainIndex <- createDataPartition(mydata$y, p = .8, list = FALSE)
#' train <- mydata[trainIndex, ]
#' test <- mydata[-trainIndex, ]
#'
#' check_multicollinearity(train, y ~ x1 + x2 + x3)
check_multicollinearity <- function(data, formula) {
vif_values <- vif(lm(formula, data = data))
vif_table <- tibble(Variable = names(vif_values), VIF = vif_values)
print(vif_table)
}
check_multicollinearity(train, y ~ x1 + x2 + x3)
#' Load and preprocess data for Recursive Feature Elimination
#'
#' This function loads the specified dataset, splits it into training and testing sets, and performs Recursive Feature Elimination (RFE) on the training set using the Random Forest algorithm. The function returns the variable importance scores computed from the RFE analysis.
#'
#' @importFrom caret createDataPartition rfeControl rfe rfFuncs
#'
#' @param data_name A character string specifying the name of the dataset to be used. The dataset must be available in R.
#'
#' @return A data frame containing the variable importance scores computed from the RFE analysis.
#'
#' @examples
#' # Load iris dataset and perform RFE
#' data(iris)
#' iris_imp <- rfe_var_imp("iris")
#'
#' # Load mtcars dataset and perform RFE
#' data(mtcars)
#' mtcars_imp <- rfe_var_imp("mtcars")
#'
#' @export
rfe_var_imp <- function(data_name) {
# Load a dataset
data(data_name)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(get(data_name)$Species, p = .8, list = FALSE)
trainData <- get(data_name)[trainIndex, ]
testData <- get(data_name)[-trainIndex, ]
# Recursive Feature Elimination (RFE)
ctrl <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
rfeProfile <- rfe(x = trainData[, -ncol(trainData)], y = trainData[, ncol(trainData)], sizes = c(1:(ncol(trainData)-1)),
rfeControl = ctrl)
# Get the variable importance scores
rfeImp <- rfeProfile$variables
# Return the variable importance scores
return(rfeImp)
}
data(iris)
iris2 <- iris[, 1:3] # Use only the first three variables for demonstration purposes
iris_imp <- rfe_var_imp("iris2")
#' Load and preprocess data for Recursive Feature Elimination
#'
#' This function loads the specified dataset, splits it into training and testing sets, and performs Recursive Feature Elimination (RFE) on the training set using the Random Forest algorithm. The function returns the variable importance scores computed from the RFE analysis.
#'
#' @importFrom caret createDataPartition rfeControl rfe rfFuncs
#'
#' @param data_name A character string specifying the name of the dataset to be used. The dataset must be available in R.
#'
#' @return A data frame containing the variable importance scores computed from the RFE analysis.
#'
#' @examples
#' # Load iris dataset and perform RFE
#' data(iris)
#' iris_imp <- rfe_var_imp("iris")
#'
#' # Load mtcars dataset and perform RFE
#' data(mtcars)
#' mtcars_imp <- rfe_var_imp("mtcars")
#'
#' @export
rfe_var_imp <- function(data_name) {
# Load the necessary packages
library(caret)
# Load a dataset
data(data_name)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(get(data_name)$Species, p = .8, list = FALSE)
trainData <- get(data_name)[trainIndex, ]
testData <- get(data_name)[-trainIndex, ]
# Recursive Feature Elimination (RFE)
ctrl <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
rfeProfile <- rfe(x = trainData[, -ncol(trainData)], y = trainData[, ncol(trainData)], sizes = c(1:(ncol(trainData)-1)),
rfeControl = ctrl)
# Get the variable importance scores
rfeImp <- rfeProfile$variables
# Return the variable importance scores
return(rfeImp)
}
iris_imp <- rfe_var_imp("iris")
#' Load and preprocess data for Recursive Feature Elimination
#'
#' This function loads the specified dataset, splits it into training and testing sets, and performs Recursive Feature Elimination (RFE) on the training set using the Random Forest algorithm. The function returns the variable importance scores computed from the RFE analysis.
#'
#' @importFrom caret createDataPartition rfeControl rfe rfFuncs
#'
#' @param data_name A character string specifying the name of the dataset to be used. The dataset must be available in R.
#'
#' @return A data frame containing the variable importance scores computed from the RFE analysis.
#'
#' @examples
#' # Load iris dataset and perform RFE
#' iris_imp <- rfe_var_imp("iris")
#'
#' # Load mtcars dataset and perform RFE
#' mtcars_imp <- rfe_var_imp("mtcars")
#'
#' @export
rfe_var_imp <- function(data_name) {
# Load the necessary packages
library(caret)
# Load a dataset
data <- get(data_name)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(data$Species, p = .8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Recursive Feature Elimination (RFE)
ctrl <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
rfeProfile <- rfe(x = trainData[, -ncol(trainData)], y = trainData[, ncol(trainData)], sizes = c(1:(ncol(trainData)-1)),
rfeControl = ctrl)
# Get the variable importance scores
rfeImp <- rfeProfile$variables
# Return the variable importance scores
return(rfeImp)
}
iris_imp <- rfe_var_imp("iris")
View(iris_imp)
